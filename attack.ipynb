{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from hpsv2.src.open_clip import create_model_and_transforms, get_tokenizer\n",
    "import warnings\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "model_dict = {}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def initialize_model():\n",
    "    if not model_dict:\n",
    "        model, preprocess_train, preprocess_val = create_model_and_transforms(\n",
    "            'ViT-H-14',\n",
    "            'laion2B-s32B-b79K',\n",
    "            precision='amp',\n",
    "            device=device,\n",
    "            jit=False,\n",
    "            force_quick_gelu=False,\n",
    "            force_custom_text=False,\n",
    "            force_patch_dropout=False,\n",
    "            force_image_size=None,\n",
    "            pretrained_image=False,\n",
    "            image_mean=None,\n",
    "            image_std=None,\n",
    "            light_augmentation=True,\n",
    "            aug_cfg={},\n",
    "            output_dict=True,\n",
    "            with_score_predictor=False,\n",
    "            with_region_predictor=False\n",
    "        )\n",
    "        model_dict['model'] = model\n",
    "        model_dict['preprocess_val'] = preprocess_val\n",
    "        \n",
    "initialize_model()\n",
    "model = model_dict['model']\n",
    "preprocess_val = model_dict['preprocess_val']\n",
    "\n",
    "checkpoint = torch.load(\"HPS_v2_compressed.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "tokenizer = get_tokenizer('ViT-H-14')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path: str, prompt: str):\n",
    "    # Process the image\n",
    "    image = preprocess_val(Image.open(file_path)).unsqueeze(0).to(device=device, non_blocking=True)\n",
    "    # Process the prompt\n",
    "    text = tokenizer([prompt]).to(device=device, non_blocking=True)\n",
    "    return image, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateHPS(image, text):\n",
    "    with torch.no_grad():\n",
    "        print(\"Calculating HPS...\")\n",
    "        # Calculate the HPS\n",
    "        with torch.amp.autocast(\"cuda\"):          \n",
    "            # from train.py\n",
    "            output = model(image, text)\n",
    "            image_features, text_features, logit_scale = output[\"image_features\"], output[\"text_features\"], output[\"logit_scale\"]\n",
    "            logits_per_text = logit_scale * text_features @ image_features.T\n",
    "            hps_score = torch.diagonal(logits_per_text).cpu().numpy()\n",
    "            \n",
    "        print(hps_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from the torchattacks source code\n",
    "class PGD():\n",
    "    def __init__(self, model, eps=8 / 255, alpha=2 / 255, steps=10):\n",
    "        self.model = model\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.steps = steps\n",
    "\n",
    "    def forward(self, images, texts):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "\n",
    "        image_adv = images.clone().detach()\n",
    "        image_adv.requires_grad = True\n",
    "        \n",
    "        mean = torch.tensor(getattr(self.model.visual, \"image_mean\")).view(1, 3, 1, 1).to(device)\n",
    "        std = torch.tensor(getattr(self.model.visual, \"image_std\")).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "        eps_norm = self.eps / std\n",
    "\n",
    "        for i in range(self.steps):\n",
    "            self.model.zero_grad()\n",
    "            \n",
    "            with torch.amp.autocast(\"cuda\"):          \n",
    "                # from train.py\n",
    "                output = self.model(image_adv, texts)\n",
    "                image_features, text_features, logit_scale = output[\"image_features\"], output[\"text_features\"], output[\"logit_scale\"]\n",
    "                logits_per_text = logit_scale * text_features @ image_features.T\n",
    "                hps_score = torch.diagonal(logits_per_text)[0]\n",
    "            \n",
    "            # We want to minimize 'score'\n",
    "            loss = hps_score  # since score is a scalar, treat it as the loss to minimize\n",
    "            loss.backward()  # compute gradients dloss/dimage\n",
    "            \n",
    "            grad = image_adv.grad\n",
    "            image_adv = image_adv - self.alpha * torch.sign(grad)\n",
    "            \n",
    "            delta = image_adv - images\n",
    "            delta = torch.max(torch.min(delta, eps_norm), -eps_norm)\n",
    "            image_adv = images + delta\n",
    "            \n",
    "            min_norm = (0 - mean) / std\n",
    "            max_norm = (1 - mean) / std\n",
    "            image_adv = torch.max(torch.min(image_adv, max_norm), min_norm)\n",
    "            \n",
    "            image_adv = image_adv.detach()\n",
    "            image_adv.requires_grad = True\n",
    "\n",
    "        return image_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the non attacked image\n",
    "filepath = \"cat.png\"\n",
    "image, text = preprocess_data(filepath, \"cat\")\n",
    "calculateHPS(image, text)\n",
    "\n",
    "# Now convert and save the preprocessed image to disk\n",
    "to_pil = transforms.ToPILImage()\n",
    "img_pil = to_pil(image[0].detach().cpu())\n",
    "img_pil.save(f\"processed_{filepath.split('/')[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = PGD(model, eps=8/(255*1), alpha=1/(255*1), steps=10)\n",
    "adv_image = attack.forward(image, text)\n",
    "print(f'Created {len(adv_image)} adversarial images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate HPS of the adversarial image\n",
    "calculateHPS(adv_image, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt function that takes the preprocessed image and takes it back to a normal image\n",
    "def denormalize(image, mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)):\n",
    "    \"\"\"\n",
    "    Reverses the CLIP preprocessing normalization.\n",
    "    \n",
    "    Args:\n",
    "        image (torch.Tensor): A normalized image tensor of shape (C, H, W) or (N, C, H, W).\n",
    "        mean (tuple): The mean values used for normalization.\n",
    "        std (tuple): The std values used for normalization.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The denormalized image with pixel values in [0, 1].\n",
    "    \"\"\"\n",
    "    if image.dim() == 4:\n",
    "        # For a batch of images\n",
    "        mean_tensor = torch.tensor(mean).view(1, 3, 1, 1).to(image.device)\n",
    "        std_tensor = torch.tensor(std).view(1, 3, 1, 1).to(image.device)\n",
    "    else:\n",
    "        # For a single image\n",
    "        mean_tensor = torch.tensor(mean).view(3, 1, 1).to(image.device)\n",
    "        std_tensor = torch.tensor(std).view(3, 1, 1).to(image.device)\n",
    "    \n",
    "    # Reverse the normalization: x = x_norm * std + mean\n",
    "    image = image * std_tensor + mean_tensor\n",
    "    # Optionally, clip to [0, 1] if needed.\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm_adv_image = denormalize(adv_image, mean=getattr(model.visual, \"image_mean\"), std=getattr(model.visual, \"image_std\"))\n",
    "img_pil = to_pil(denorm_adv_image.squeeze().cpu().detach())\n",
    "img_pil.save(f'adv_image.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
